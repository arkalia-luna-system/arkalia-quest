#!/usr/bin/env python3
"""
üõ°Ô∏è TESTS DE ROBUSTESSE AVANC√âE - ARKALIA QUEST
Tests de robustesse, r√©silience et gestion d'erreurs critiques
PRIORIT√â ABSOLUE - Garantir la stabilit√© du syst√®me en production
"""

import concurrent.futures
import os
import random
import sys
import tempfile
import threading
import time
import unittest

# Ajouter le r√©pertoire parent au path
sys.path.insert(
    0,
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
)

from utils.logger import GameLogger  # noqa: E402

# Initialiser le logger
game_logger = GameLogger()

try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    game_logger.info(r"‚ö†Ô∏è psutil non disponible - tests de m√©moire limit√©s")

try:
    from core.database import DatabaseManager
    from core.educational_games_engine import EducationalGamesEngine
    from core.gamification_engine import GamificationEngine
    from core.luna_emotions_engine import LunaEmotionsEngine
    from core.security_manager import SecurityManager
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    sys.exit(1)


class TestRobustesseAvancee(unittest.TestCase):
    """Tests de robustesse avanc√©e pour Arkalia Quest"""

    def setUp(self):
        """Initialisation avant chaque test"""
        game_logger.info(r"üõ°Ô∏è Initialisation des tests de robustesse avanc√©e...")

        # Cr√©er un r√©pertoire temporaire pour les tests
        self.test_dir = tempfile.mkdtemp()

        # Initialiser tous les moteurs critiques
        self.luna_engine = LunaEmotionsEngine()
        self.gamification_engine = GamificationEngine(data_dir=self.test_dir)
        self.db_manager = DatabaseManager()
        self.educational_engine = EducationalGamesEngine()
        self.security_manager = SecurityManager()

        # Profils de test vari√©s pour tester diff√©rents sc√©narios
        self.test_profiles = {
            "debutant": {"level": 1, "score": 0, "badges": []},
            "intermediaire": {"level": 5, "score": 1500, "badges": ["hacker"]},
            "expert": {"level": 10, "score": 5000, "badges": ["maitre", "speedrunner"]},
            "extreme": {
                "level": 100,
                "score": 100000,
                "badges": ["legend", "ultimate"],
            },
        }

        # M√©triques de robustesse
        self.robustness_metrics = {
            "error_count": 0,
            "recovery_success": 0,
            "performance_degradation": [],
            "memory_leaks": [],
            "concurrent_operations": [],
        }

        game_logger.info(r"‚úÖ Initialisation termin√©e")

    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        # Nettoyer les ressources
        if hasattr(self, "test_dir") and os.path.exists(self.test_dir):
            import shutil

            shutil.rmtree(self.test_dir)

        # R√©initialiser les moteurs
        self.luna_engine.reset_emotions()

        game_logger.info(f"üßπ Nettoyage termin√© - M√©triques: {self.robustness_metrics}")

    def test_resilience_under_extreme_load(self):
        """Test de r√©silience sous charge extr√™me"""
        game_logger.info(r"üî• Test de r√©silience sous charge extr√™me...")

        # Simuler 1000 op√©rations simultan√©es
        operations = []
        start_time = time.time()

        with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
            # Cr√©er des t√¢ches vari√©es
            for i in range(1000):
                profile = random.choice(list(self.test_profiles.values()))
                action = f"action_{i % 10}"
                result = {
                    "r√©ussite": i % 2 == 0,
                    "score_gagne": random.randint(0, 1000),
                }

                future = executor.submit(
                    self._execute_operation_safely,
                    self.luna_engine.analyze_action,
                    action,
                    result,
                    profile,
                )
                operations.append(future)

            # Attendre la completion avec timeout
            completed = 0
            errors = 0

            for future in concurrent.futures.as_completed(operations, timeout=30):
                try:
                    result = future.result(timeout=1)
                    completed += 1
                except Exception as e:
                    errors += 1
                    self.robustness_metrics["error_count"] += 1
                    print(f"‚ö†Ô∏è Erreur dans l'op√©ration: {e}")

        end_time = time.time()
        duration = end_time - start_time

        # V√©rifications de robustesse
        self.assertGreater(completed, 950, "Trop d'op√©rations ont √©chou√©")
        self.assertLess(errors, 50, "Trop d'erreurs sous charge")
        self.assertLess(duration, 35, "Performance d√©grad√©e sous charge")

        game_logger.info(f"‚úÖ R√©silience: {completed}/1000 op√©rations r√©ussies en {duration:.2f}s")
        game_logger.info(f"üìä Erreurs: {errors}, M√©triques: {self.robustness_metrics}")

    def test_memory_leak_prevention(self):
        """Test de pr√©vention des fuites m√©moire"""
        game_logger.info(r"üíæ Test de pr√©vention des fuites m√©moire...")

        if not PSUTIL_AVAILABLE:
            self.skipTest("psutil non disponible")

        process = psutil.Process()
        initial_memory = process.memory_info().rss

        # Simuler une longue session de jeu
        for session in range(10):
            session_memory = []

            for i in range(100):
                # Op√©rations intensives
                profile = random.choice(list(self.test_profiles.values()))
                for _ in range(50):
                    self.luna_engine.analyze_action(
                        f"session_{session}_action_{i}",
                        {"r√©ussite": True, "score_gagne": random.randint(0, 100)},
                        profile,
                    )

                # V√©rifier la m√©moire
                current_memory = process.memory_info().rss
                session_memory.append(current_memory)

            # V√©rifier qu'il n'y a pas de fuite progressive
            if session > 0:
                memory_increase = session_memory[-1] - session_memory[0]
                self.assertLess(
                    memory_increase,
                    10 * 1024 * 1024,  # 10MB max
                    f"Fuite m√©moire d√©tect√©e dans la session {session}",
                )

        final_memory = process.memory_info().rss
        total_increase = final_memory - initial_memory

        # V√©rifier que l'augmentation totale est raisonnable
        self.assertLess(
            total_increase,
            50 * 1024 * 1024,  # 50MB max
            "Fuite m√©moire globale d√©tect√©e",
        )

        print(
            f"‚úÖ M√©moire: {initial_memory / 1024 / 1024:.1f}MB ‚Üí {final_memory / 1024 / 1024:.1f}MB",
        )
        game_logger.info(f"üìä Augmentation: {total_increase / 1024 / 1024:.1f}MB")

    def test_error_recovery_mechanisms(self):
        """Test des m√©canismes de r√©cup√©ration d'erreurs"""
        print("üîÑ Test des m√©canismes de r√©cup√©ration d'erreurs...")

        # Tester la r√©cup√©ration apr√®s des erreurs critiques
        recovery_tests = [
            self._test_database_recovery,
            self._test_engine_recovery,
            self._test_memory_recovery,
            self._test_concurrent_recovery,
        ]

        for test_func in recovery_tests:
            try:
                test_func()
                self.robustness_metrics["recovery_success"] += 1
            except Exception as e:
                game_logger.info(f"‚ùå √âchec du test de r√©cup√©ration {test_func.__name__}: {e}")
                self.robustness_metrics["error_count"] += 1

        # V√©rifier que la plupart des r√©cup√©rations r√©ussissent
        success_rate = self.robustness_metrics["recovery_success"] / len(recovery_tests)
        self.assertGreater(success_rate, 0.8, "Taux de r√©cup√©ration trop faible")

        print(
            f"‚úÖ R√©cup√©ration: {self.robustness_metrics['recovery_success']}/{len(recovery_tests)}",
        )
        game_logger.info(f"üìä Taux de succ√®s: {success_rate:.1%}")

    def test_concurrent_safety(self):
        """Test de s√©curit√© concurrente"""
        game_logger.info(r"üîí Test de s√©curit√© concurrente...")

        # Tester l'acc√®s concurrent aux ressources partag√©es
        shared_data = {"counter": 0, "data": []}
        lock = threading.Lock()

        def concurrent_operation(thread_id):
            """Op√©ration concurrente qui modifie des donn√©es partag√©es"""
            for i in range(100):
                with lock:
                    shared_data["counter"] += 1
                    shared_data["data"].append(f"thread_{thread_id}_op_{i}")

                # Simuler du travail
                time.sleep(0.001)

        # Lancer 10 threads simultan√©ment
        threads = []
        for i in range(10):
            thread = threading.Thread(target=concurrent_operation, args=(i,))
            threads.append(thread)
            thread.start()

        # Attendre la completion
        for thread in threads:
            thread.join()

        # V√©rifier l'int√©grit√© des donn√©es
        expected_counter = 10 * 100
        self.assertEqual(
            shared_data["counter"],
            expected_counter,
            "Corruption des donn√©es concurrentes",
        )
        self.assertEqual(
            len(shared_data["data"]),
            expected_counter,
            "Perte de donn√©es concurrentes",
        )

        print(f"‚úÖ Concurrence: {shared_data['counter']} op√©rations thread-safe")
        print(f"üìä Donn√©es: {len(shared_data['data'])} √©l√©ments pr√©serv√©s")

    def test_extreme_edge_cases(self):
        """Test des cas limites extr√™mes"""
        game_logger.info(r"üéØ Test des cas limites extr√™mes...")

        edge_cases = [
            # Profils extr√™mes
            {"level": 0, "score": -1000, "badges": None},
            {"level": 999999, "score": 999999999, "badges": ["x" * 1000]},
            {"level": "invalid", "score": "string", "badges": 123},
            # Actions extr√™mes
            ("", {}),
            ("x" * 10000, {"r√©ussite": None, "score_gagne": "invalid"}),
            (None, None),
            # R√©sultats extr√™mes
            ("normal_action", {"r√©ussite": True, "score_gagne": float("inf")}),
            ("normal_action", {"r√©ussite": False, "score_gagne": float("-inf")}),
            ("normal_action", {"r√©ussite": "maybe", "score_gagne": complex(1, 1)}),
        ]

        errors_handled = 0

        for i, edge_case in enumerate(edge_cases):
            try:
                if isinstance(edge_case, tuple):
                    action, result = edge_case
                    profile = self.test_profiles["debutant"]
                else:
                    action = "test_action"
                    result = {"r√©ussite": True, "score_gagne": 10}
                    profile = edge_case

                # Tenter l'op√©ration
                self.luna_engine.analyze_action(action, result, profile)
                errors_handled += 1

            except Exception as e:
                # L'erreur est attendue pour les cas limites
                errors_handled += 1
                game_logger.info(f"‚úÖ Cas limite {i} g√©r√©: {type(e).__name__}")

        # V√©rifier que tous les cas limites sont g√©r√©s
        self.assertEqual(errors_handled, len(edge_cases), "Cas limites non g√©r√©s")

        game_logger.info(f"‚úÖ Cas limites: {errors_handled}/{len(edge_cases)} g√©r√©s correctement")

    def _execute_operation_safely(self, func, *args, **kwargs):
        """Ex√©cute une op√©ration avec gestion d'erreur s√©curis√©e"""
        try:
            return func(*args, **kwargs)
        except Exception as e:
            # Logger l'erreur mais ne pas la faire remonter
            game_logger.info(f"‚ö†Ô∏è Op√©ration √©chou√©e: {e}")
            return {"error": str(e), "status": "failed"}

    def _test_database_recovery(self):
        """Test de r√©cup√©ration de la base de donn√©es"""
        # Cr√©er un profil de test avec des donn√©es valides
        test_profile = {
            "username": "recovery_test",
            "score": 100,
            "level": 5,
            "badges": ["test_badge"],
            "avatars": [],
            "preferences": {"theme": "dark"},
        }

        # Sauvegarder le profil
        success = self.db_manager.save_profile("recovery_test", test_profile)
        self.assertTrue(success, "Sauvegarde du profil √©chou√©e")

        # V√©rifier la r√©cup√©ration
        recovered_data = self.db_manager.load_profile("recovery_test")
        self.assertIsNotNone(recovered_data, "Profil non r√©cup√©r√©")

        # V√©rifier que les donn√©es cl√©s sont pr√©sentes
        self.assertEqual(recovered_data["username"], "recovery_test", "Username incorrect")
        self.assertEqual(recovered_data["score"], 100, "Score incorrect")
        self.assertEqual(recovered_data["level"], 5, "Niveau incorrect")
        self.assertIn("test_badge", recovered_data["badges"], "Badge de test manquant")

        # V√©rifier que c'est un profil valide
        self.assertIsInstance(recovered_data, dict, "Profil non valide")

    def _test_engine_recovery(self):
        """Test de r√©cup√©ration des moteurs"""
        # Forcer une erreur dans le moteur LUNA
        self.luna_engine.reset_emotions()

        # V√©rifier que le moteur fonctionne toujours
        result = self.luna_engine.analyze_action(
            "recovery_test",
            {"r√©ussite": True, "score_gagne": 100},
            self.test_profiles["debutant"],
        )
        self.assertIsNotNone(result, "Moteur non r√©cup√©r√©")

    def _test_memory_recovery(self):
        """Test de r√©cup√©ration m√©moire"""
        if not PSUTIL_AVAILABLE:
            return

        # Forcer le garbage collection
        import gc

        gc.collect()

        # V√©rifier que la m√©moire est stable
        process = psutil.Process()
        memory_before = process.memory_info().rss

        # Op√©ration intensive
        for _ in range(1000):
            self.luna_engine.analyze_action(
                "memory_test",
                {"r√©ussite": True, "score_gagne": 10},
                self.test_profiles["debutant"],
            )

        gc.collect()
        memory_after = process.memory_info().rss

        # V√©rifier que la m√©moire n'explose pas
        increase = memory_after - memory_before
        self.assertLess(increase, 100 * 1024 * 1024, "R√©cup√©ration m√©moire √©chou√©e")

    def _test_concurrent_recovery(self):
        """Test de r√©cup√©ration concurrente"""

        # Simuler des erreurs concurrentes
        def error_operation():
            try:
                # Op√©ration qui peut √©chouer
                if random.random() < 0.3:
                    raise ValueError("Erreur simul√©e")
                return "success"
            except Exception:
                return "recovered"

        # Ex√©cuter en parall√®le
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(error_operation) for _ in range(20)]
            results = [f.result() for f in futures]

        # V√©rifier que toutes les op√©rations se terminent
        self.assertEqual(len(results), 20, "R√©cup√©ration concurrente √©chou√©e")


def run_robustness_tests():
    """Lance tous les tests de robustesse"""
    game_logger.info(r"üõ°Ô∏è LANCEMENT DES TESTS DE ROBUSTESSE AVANC√âE")
    print("=" * 60)

    # Cr√©er la suite de tests
    test_suite = unittest.TestSuite()
    test_suite.addTest(unittest.makeSuite(TestRobustesseAvancee))

    # Ex√©cuter les tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)

    # R√©sum√© d√©taill√©
    print("\n" + "=" * 60)
    game_logger.info(r"üìä R√âSUM√â DES TESTS DE ROBUSTESSE")
    game_logger.info(f"Tests ex√©cut√©s: {result.testsRun}")
    game_logger.info(f"Succ√®s: {result.testsRun - len(result.failures) - len(result.errors)}")
    game_logger.info(f"√âchecs: {len(result.failures)}")
    game_logger.info(f"Erreurs: {len(result.errors)}")

    if result.failures:
        game_logger.info(r"\n‚ùå √âCHECS:")
        for test, traceback in result.failures:
            game_logger.info(f"  - {test}: {traceback}")

    if result.errors:
        game_logger.info(r"\nüí• ERREURS:")
        for test, traceback in result.errors:
            game_logger.info(f"  - {test}: {traceback}")

    success_rate = (
        (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun
    ) * 100
    game_logger.info(f"\nüéØ TAUX DE R√âUSSITE: {success_rate:.1f}%")

    if success_rate >= 90:
        game_logger.info(r"üåü SYST√àME TR√àS ROBUSTE - Pr√™t pour la production !")
    elif success_rate >= 80:
        game_logger.info(r"‚úÖ SYST√àME ROBUSTE - Quelques am√©liorations recommand√©es")
    else:
        game_logger.info(r"‚ö†Ô∏è SYST√àME PEU ROBUSTE - Actions correctives n√©cessaires")

    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_robustness_tests()
    sys.exit(0 if success else 1)
