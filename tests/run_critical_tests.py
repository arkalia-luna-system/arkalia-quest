#!/usr/bin/env python3
"""
üöÄ LANCEUR DES TESTS CRITIQUES - ARKALIA QUEST
Lance tous les tests critiques pour valider la qualit√© du jeu
PRIORIT√â ABSOLUE - Validation de la stabilit√© et des performances
"""

import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

# Configuration des tests
TEST_CONFIG = {
    "base_url": "http://localhost:5001",
    "timeout": 30,
    "parallel": False,
    "generate_reports": True,
    "coverage_target": 85,
}

# Structure des tests par priorit√©s
TEST_PRIORITIES = {
    "üî• CRITIQUE": {
        "description": "Tests des modules critiques et de la stabilit√©",
        "tests": [
            "tests/core/test_luna_emotions_complete.py",
            "tests/core/test_gamification_complete.py",
            "tests/performance/test_performance_complete.py",
        ],
        "required": True,
    },
    "‚ö° HAUTE PRIORIT√â": {
        "description": "Tests d'interface et d'accessibilit√©",
        "tests": [
            "tests/test_accessibility_complete.py",
            "tests/test_ui_tutoriel_experience.py",
            "tests/test_ui_terminal_experience.py",
        ],
        "required": True,
    },
    "üì± PRIORIT√â MOYENNE": {
        "description": "Tests d'exp√©rience utilisateur",
        "tests": [
            "tests/test_ui_improvements_teen.py",
            "tests/test_immersive_system_complete.py",
            "tests/test_ui_pwa_mobile_experience.py",
        ],
        "required": False,
    },
}

# Couleurs pour l'affichage
COLORS = {
    "reset": "\033[0m",
    "bold": "\033[1m",
    "red": "\033[91m",
    "green": "\033[92m",
    "yellow": "\033[93m",
    "blue": "\033[94m",
    "magenta": "\033[95m",
    "cyan": "\033[96m",
}


def print_header():
    """Affiche l'en-t√™te du lanceur de tests"""
    print(f"{COLORS['bold']}{COLORS['cyan']}")
    print("=" * 80)
    game_logger.info(r"üöÄ LANCEUR DES TESTS CRITIQUES - ARKALIA QUEST")
    print("=" * 80)
    print(f"{COLORS['reset']}")
    print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    game_logger.info(r"üéØ Objectif: Validation compl√®te de la qualit√© du jeu")
    print(f"üìä Couverture cible: {TEST_CONFIG['coverage_target']}%")
    print(f"‚ö° Mode parall√®le: {'Activ√©' if TEST_CONFIG['parallel'] else 'D√©sactiv√©'}")
    print()


def check_environment():
    """V√©rifie l'environnement de test"""
    print(f"{COLORS['blue']}üîç V√âRIFICATION DE L'ENVIRONNEMENT{COLORS['reset']}")
    print("-" * 50)

    # V√©rifier Python
    python_version = sys.version_info
    game_logger.info(
        f"üêç Python: {python_version.major}.{python_version.minor}.{python_version.micro}"
    )

    if python_version < (3, 8):
        print(f"{COLORS['red']}‚ùå Python 3.8+ requis{COLORS['reset']}")
        return False
    print(f"{COLORS['green']}‚úÖ Version Python compatible{COLORS['reset']}")

    # V√©rifier les d√©pendances
    required_packages = ["pytest", "black", "ruff"]
    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
            print(f"{COLORS['green']}‚úÖ {package} disponible{COLORS['reset']}")
        except ImportError:
            print(f"{COLORS['red']}‚ùå {package} manquant{COLORS['reset']}")
            missing_packages.append(package)

    if missing_packages:
        print(
            f"\n{COLORS['yellow']}‚ö†Ô∏è Installez les packages"
            + "manquants:{COLORS['reset']}"
        )
        print(f"pip install {' '.join(missing_packages)}")
        return False

    # V√©rifier la structure des tests
    test_dir = Path("tests")
    if not test_dir.exists():
        print(f"{COLORS['red']}‚ùå Dossier tests/ manquant{COLORS['reset']}")
        return False

    print(f"{COLORS['green']}‚úÖ Structure des tests valide{COLORS['reset']}")

    print()
    return True


def run_code_quality_checks():
    """Ex√©cute les v√©rifications de qualit√© du code"""
    print(f"{COLORS['blue']}üîß V√âRIFICATIONS DE QUALIT√â DU CODE{COLORS['reset']}")
    print("-" * 50)

    # V√©rifier le formatage avec Black
    game_logger.info(r"üé® V√©rification du formatage avec Black...")
    try:
        result = subprocess.run(
            ["black", "--check", "."],
            check=False,
            capture_output=True,
            text=True,
            timeout=60,
        )

        if result.returncode == 0:
            print(f"{COLORS['green']}‚úÖ Code correctement format√©{COLORS['reset']}")
        else:
            print(
                f"{COLORS['yellow']}‚ö†Ô∏è Code non format√©, application de"
                "Black...{COLORS['reset']}",
            )
            subprocess.run(["black", "."], check=True)
            print(f"{COLORS['green']}‚úÖ Formatage appliqu√©{COLORS['reset']}")
    except subprocess.TimeoutExpired:
        print(
            f"{COLORS['red']}‚ùå Timeout lors de la v√©rification Black{COLORS['reset']}"
        )
    except Exception as e:
        print(f"{COLORS['red']}‚ùå Erreur Black: {e}{COLORS['reset']}")

    # V√©rifier la qualit√© avec Ruff
    game_logger.info(r"üßπ V√©rification de la qualit√© avec Ruff...")
    try:
        result = subprocess.run(
            ["ruff", "check", "."],
            check=False,
            capture_output=True,
            text=True,
            timeout=60,
        )

        if result.returncode == 0:
            print(f"{COLORS['green']}‚úÖ Code conforme aux standards{COLORS['reset']}")
        else:
            print(
                f"{COLORS['yellow']}‚ö†Ô∏è  Probl√®mes de qualit√© d√©tect√©s{COLORS['reset']}"
            )
            print(result.stdout)
            print(f"{COLORS['yellow']}‚ö†Ô∏è  Correction automatique...{COLORS['reset']}")
            subprocess.run(["ruff", "check", "--fix", "."], check=True)
            print(f"{COLORS['green']}‚úÖ Corrections appliqu√©es{COLORS['reset']}")
    except subprocess.TimeoutExpired:
        print(
            f"{COLORS['red']}‚ùå Timeout lors de la v√©rification Ruff{COLORS['reset']}"
        )
    except Exception as e:
        print(f"{COLORS['red']}‚ùå Erreur Ruff: {e}{COLORS['reset']}")

    print()


def run_test_category(category_name, category_info):
    """Ex√©cute une cat√©gorie de tests"""
    print(
        f"{COLORS['blue']}{category_name} -"
        + "{category_info['description']}{COLORS['reset']}"
    )
    print("-" * 60)

    results = {
        "total": 0,
        "passed": 0,
        "failed": 0,
        "errors": 0,
        "duration": 0,
    }

    start_time = time.time()

    for test_file in category_info["tests"]:
        if not os.path.exists(test_file):
            print(
                f"{COLORS['yellow']}‚ö†Ô∏è Fichier de test manquant:"
                + "{test_file}{COLORS['reset']}"
            )
            continue

        game_logger.info(f"üß™ Ex√©cution: {test_file}")

        try:
            # Ex√©cuter le test avec pytest
            cmd = [
                "python",
                "-m",
                "pytest",
                test_file,
                "-v",
                "--tb=short",
                "--strict-markers",
                "--disable-warnings",
            ]

            if TEST_CONFIG["parallel"]:
                cmd.extend(["-n", "auto"])

            result = subprocess.run(
                cmd,
                check=False,
                capture_output=True,
                text=True,
                timeout=TEST_CONFIG["timeout"] * 60,
            )

            # Analyser le r√©sultat
            if result.returncode == 0:
                print(f"{COLORS['green']}‚úÖ Test r√©ussi{COLORS['reset']}")
                results["passed"] += 1
            else:
                print(f"{COLORS['red']}‚ùå Test √©chou√©{COLORS['reset']}")
                print(result.stdout)
                print(result.stderr)
                results["failed"] += 1

            results["total"] += 1

        except subprocess.TimeoutExpired:
            print(f"{COLORS['red']}‚ùå Timeout du test{COLORS['reset']}")
            results["errors"] += 1
            results["total"] += 1
        except Exception as e:
            print(f"{COLORS['red']}‚ùå Erreur d'ex√©cution: {e}{COLORS['reset']}")
            results["errors"] += 1
            results["total"] += 1

    end_time = time.time()
    results["duration"] = end_time - start_time

    # Afficher le r√©sum√© de la cat√©gorie
    game_logger.info(f"\nüìä R√©sum√© {category_name}:")
    print(f"   Total: {results['total']}")
    print(f"   R√©ussis: {results['passed']}")
    print(f"   √âchou√©s: {results['failed']}")
    print(f"   Erreurs: {results['errors']}")
    print(f"   Dur√©e: {results['duration']:.2f}s")

    # V√©rifier si la cat√©gorie est critique
    if category_info["required"]:
        success_rate = (
            results["passed"] / results["total"] if results["total"] > 0 else 0
        )
        if success_rate < 0.8:  # 80% de succ√®s minimum
            print(f"{COLORS['red']}‚ùå Cat√©gorie critique √©chou√©e{COLORS['reset']}")
            return False

    print()
    return True


def run_coverage_analysis():
    """Ex√©cute l'analyse de couverture"""
    print(f"{COLORS['blue']}üìä ANALYSE DE COUVERTURE{COLORS['reset']}")
    print("-" * 50)

    try:
        # Ex√©cuter les tests avec couverture
        cmd = [
            "python",
            "-m",
            "pytest",
            "--cov=core",
            "--cov=arkalia_engine",
            "--cov=app",
            "--cov-report=html",
            "--cov-report=term-missing",
            "--cov-fail-under",
            str(TEST_CONFIG["coverage_target"]),
            "tests/core/",
            "tests/performance/",
        ]

        result = subprocess.run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
            timeout=300,  # 5 minutes pour la couverture
        )

        if result.returncode == 0:
            print(f"{COLORS['green']}‚úÖ Couverture atteinte{COLORS['reset']}")
        else:
            print(f"{COLORS['yellow']}‚ö†Ô∏è  Couverture insuffisante{COLORS['reset']}")
            print(result.stdout)

        # Afficher le rapport de couverture
        game_logger.info(r"\nüìà Rapport de couverture:")
        print(result.stdout)

    except subprocess.TimeoutExpired:
        print(f"{COLORS['red']}‚ùå Timeout de l'analyse de couverture{COLORS['reset']}")
    except Exception as e:
        print(f"{COLORS['red']}‚ùå Erreur d'analyse de couverture: {e}{COLORS['reset']}")

    print()


def generate_test_report():
    """G√©n√®re un rapport de test complet"""
    print(f"{COLORS['blue']}üìã G√âN√âRATION DU RAPPORT{COLORS['reset']}")
    print("-" * 50)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"tests/reports/test_report_{timestamp}.json"

    # Cr√©er le dossier reports s'il n'existe pas
    os.makedirs("tests/reports", exist_ok=True)

    # Donn√©es du rapport
    report_data = {
        "timestamp": datetime.now().isoformat(),
        "test_config": TEST_CONFIG,
        "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
        "test_priorities": TEST_PRIORITIES,
        "summary": {
            "total_categories": len(TEST_PRIORITIES),
            "critical_tests": sum(
                len(cat["tests"]) for cat in TEST_PRIORITIES.values() if cat["required"]
            ),
            "optional_tests": sum(
                len(cat["tests"])
                for cat in TEST_PRIORITIES.values()
                if not cat["required"]
            ),
        },
    }

    # Sauvegarder le rapport
    try:
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"{COLORS['green']}‚úÖ Rapport g√©n√©r√©: {report_file}{COLORS['reset']}")
    except Exception as e:
        print(
            f"{COLORS['red']}‚ùå Erreur de g√©n√©ration du rapport: {e}{COLORS['reset']}"
        )

    print()


def main():
    """Fonction principale du lanceur de tests"""
    print_header()

    # V√©rifier l'environnement
    if not check_environment():
        print(
            f"{COLORS['red']}‚ùå Environnement invalide, arr√™t des tests{COLORS['reset']}"
        )
        sys.exit(1)

    # Ex√©cuter les v√©rifications de qualit√©
    run_code_quality_checks()

    # Ex√©cuter les tests par cat√©gorie
    all_tests_passed = True

    for category_name, category_info in TEST_PRIORITIES.items():
        category_success = run_test_category(category_name, category_info)
        if not category_success and category_info["required"]:
            all_tests_passed = False

    # Ex√©cuter l'analyse de couverture
    run_coverage_analysis()

    # G√©n√©rer le rapport
    if TEST_CONFIG["generate_reports"]:
        generate_test_report()

    # Affichage du r√©sum√© final
    print(f"{COLORS['bold']}{COLORS['cyan']}üéØ R√âSUM√â FINAL{COLORS['reset']}")
    print("=" * 80)

    if all_tests_passed:
        print(
            f"{COLORS['green']}üéâ TOUS LES TESTS CRITIQUES ONT R√âUSSI !{COLORS['reset']}"
        )
        print(
            f"{COLORS['green']}‚úÖ Arkalia Quest est pr√™t pour la"
            + "production{COLORS['reset']}"
        )
        sys.exit(0)
    else:
        print(f"{COLORS['red']}‚ùå CERTAINS TESTS CRITIQUES ONT √âCHOU√â{COLORS['reset']}")
        print(
            f"{COLORS['yellow']}‚ö†Ô∏è V√©rifiez les erreurs avant le"
            + "d√©ploiement{COLORS['reset']}"
        )
        sys.exit(1)


if __name__ == "__main__":
    main()
